# Feature 04 ‚Äì Output Simulator: Quick-Start On-Boarding Guide

**Feature:** Output Simulator (Artifact Preview & Validation)  
**Version:** 1.2  
**Last Updated:** 2026-02-09

<!--
  Version History:
  - 1.0 (2026-02-09): Initial on-boarding guide creation
  - 1.1 (2026-02-09): Added Windows/WSL compatibility notes, debugging guide, continuation steps
  - 1.2 (2026-02-09): UI integration complete - panels rendering correctly, tested and verified
-->

## Version Updates

- **2026-02-09 (1.2):** 
  - ‚úÖ **UI INTEGRATION COMPLETE** - Rule visualization and simulation panels now display correctly
  - Fixed layout issues with flex/grid panel sizing (`h-96` explicit height for grid container)
  - Fixed rule card click handlers - cards now properly select and display panels
  - Verified artifact preview panel appears after running simulation
  - All UI components working end-to-end  

---

## What This Feature Does

The Output Simulator lets you **preview generated artifacts before execution**. After you run a simulation in the Orchestrator Hub, a right-sidebar panel shows mock artifacts (code, HTML, JSON, SQL, configs) with syntax highlighting, validation, and comparison tools. You can inspect, validate, and modify simulation parameters without committing to full execution.

---

## ‚úÖ STATUS ‚Äì ALL SYSTEMS GO

### Feature Completion Status
- ‚úÖ **UI Components:** Fully integrated and rendering
- ‚úÖ **Rule Management:** Create, select, edit rules
- ‚úÖ **Simulation Panel:** Displays with constraints (Max Agents, Cost Budget sliders)
- ‚úÖ **Orchestrator Engine:** Working with proper null safety
- ‚úÖ **Artifact Preview Panel:** Renders on right sidebar after simulation
- ‚úÖ **Artifact Tabs:** Code, HTML, JSON, SQL, Config, Test
- ‚úÖ **Copy/Export:** Working for all artifact types
- ‚úÖ **Validation:** Artifacts show validation badges

### Bug Fixes Applied (2026-02-09)
**NullReferenceException ‚Äì FIXED ‚úÖ**
- Root Cause: Missing null checks in orchestrator engine
- Fix: Added optional chaining (`?.`) in `/lib/orchestrator-engine.ts`
- Status: Verified working, no regressions

**UI Panel Layout Issues ‚Äì FIXED ‚úÖ**
- Root Cause: Flex/grid layout collapsing without explicit height
- Fix: Added `h-96` height constraint to grid container in `/app/orchestrator/page.tsx` line 163
- Status: Verified working, panels display correctly

**Rule Card Click Handlers ‚Äì FIXED ‚úÖ**
- Root Cause: Card component blocking clicks
- Fix: Moved onClick to outer div, added pointer-events-none to Card
- Status: Verified working, rule selection works

---

## Quick Start (5 min)

### Step 1: Navigate to `/orchestrator`
```
http://localhost:3000/orchestrator
```

### Step 2: Create a Rule
- Click **"New Rule"** button (top-left)
- Set **Name:** `test-rule`
- Set **Priority:** `5`
- Click **Save**

### Step 3: Select the Rule
- Click the **`test-rule`** card (left sidebar)
- ‚úÖ Rule highlights in blue
- ‚úÖ Center panels appear: **Rule Visualization** (left) + **Simulation** (right)

### Step 4: Run Simulation
- In the **Simulation** panel (center-right), click **"Run Simulation"** (blue button)
- Wait 2-3 seconds for spinner to complete

### Step 5: View Artifacts
- ‚úÖ **Artifact Preview Panel** appears on right sidebar
- See tabs: **Code | HTML | JSON | SQL | Config | Test**
- Each artifact shows:
  - Syntax highlighting
  - Validation badge (‚úÖ Valid / ‚ö†Ô∏è Warning / ‚ùå Error)
  - **Copy** button (copies to clipboard)
  - **Export** button (downloads file)

---

## Key Features

### Preview Panel Layout
```
Orchestrator Hub (main view)
    ‚îú‚îÄ Instruction graph / task assignment (left 60%)
    ‚îî‚îÄ Artifact Preview Panel (right 40%, collapsible)
       ‚îú‚îÄ [X] Collapse/Expand
       ‚îú‚îÄ Tabs: Code | HTML | JSON | SQL | Config | Test
       ‚îî‚îÄ Content area (see below)
```

### Artifact Viewer (inside each tab)

#### Code Tab
- **Syntax highlighting:** Python, JavaScript, SQL, YAML, etc.
- **Line numbers:** Click to select ranges
- **Copy button:** Copy entire artifact to clipboard
- **Export as [.py/.js/.sql]:** Download file
- **Validation badge:** ‚úÖ Valid / ‚ö†Ô∏è Warning / ‚ùå Error

#### HTML Tab
- **Live preview iframe:** Sandboxed, renders HTML/CSS in real-time
- **Source code tab:** View HTML source alongside preview
- **Validation badge:** Tag balance check

#### JSON Tab
- **Tree view:** Expandable/collapsible nodes
- **Validation badge:** Shows if JSON is valid
- **Copy/Export:** Same as Code tab

#### Other Tabs (SQL, Config, Test, Markdown, SVG)
- **Code viewer** with syntax highlighting
- **Validation badge** with type-specific checks
- **Copy/Export buttons**

---

## üîß Debugging & Continuation Guide

### Current State (as of 2026-02-09)

**What's Done:**
- ‚úÖ Artifact data model (`lib/artifact-model.ts`)
- ‚úÖ Artifact generator (`lib/artifact-generator.ts`)
- ‚úÖ Artifact validator (`lib/artifact-validator.ts`)
- ‚úÖ API route for simulation (`app/api/orchestrator/simulate/route.ts`)
- ‚úÖ Orchestrator engine core logic (`lib/orchestrator-engine.ts`)
- ‚úÖ Frontend preview components (in `components/aei/`)
- ‚úÖ Improved API error handling (JSON parsing, validation messages)
- ‚úÖ Windows/PowerShell compatible test script (`test-artifact-workflow.ps1`)

**What's Not Done:**
- ‚ùå UI Integration - Preview panel components not connected to orchestrator page
- ‚ùå Engine bug fix - NullReferenceException investigation & fix

---

### Next Steps (Priority Order)

#### **Step 1: Fix NullReferenceException Bug** (30-45 min)

**Goal:** Make `ORCHESTRATOR.simulate()` work reliably without fallback

**Instructions:**

1. **Add debugging to orchestrator engine:**
   - Edit `/lib/orchestrator-engine.ts`
   - Add `console.log` statements:
     ```typescript
     // Line ~238 (start of decompose):
     decompose(graph: InstructionNode[]): InstructionNode[] {
       console.log('[decompose] Input graph:', graph);
       const errors = this.validateGraph(graph);
       console.log('[decompose] Validation errors:', errors);
       // ... rest of method
     }
     
     // Line ~252 (start of assign):
     assign(tasks, rule_set_id, ...) {
       console.log('[assign] Tasks:', tasks);
       console.log('[assign] Rule set ID:', rule_set_id);
       const rule_set = this.rules.get(rule_set_id);
       console.log('[assign] Retrieved rule set:', rule_set);
       // ... rest of method
     }
     ```

2. **Test with the API endpoint:**
   - Start dev server: `npm run dev`
   - Open browser console: `F12` ‚Üí Console tab
   - Send test request to `/api/orchestrator/simulate`:
     ```bash
     curl -X POST http://localhost:3000/api/orchestrator/simulate \
       -H "Content-Type: application/json" \
       -d '{"instruction_graph":[{"id":"t1","type":"code_gen","description":"test"}]}'
     ```
   - Check browser console for debug logs
   - Identify where NullReferenceException is thrown

3. **Fix the issue:**
   - Once identified, fix the null check or object initialization
   - Most likely: ensure Rule object has all required fields
   - Verify: `constraints`, `dependencies`, and `agent_type_affinity` are initialized

4. **Verify fix:**
   - Test again with curl command above
   - Should get 200 response with artifacts (no fallback)
   - Check that `simulation.assignment_plan` is populated

**Files to modify:**
- `/lib/orchestrator-engine.ts` (add debug logs, fix null issue)
- Possibly `/app/api/orchestrator/simulate/route.ts` (if Rule creation needs adjustment)

---

#### **Step 2: Integrate UI Components** (45-60 min)

**Goal:** Connect artifact preview panel to orchestrator page so it displays

**Instructions:**

1. **Import components in `/app/orchestrator/page.tsx`:**
   ```typescript
   import ArtifactPreviewPanel from '@/components/aei/artifact-preview-panel';
   ```

2. **Add state for simulation results:**
   ```typescript
   const [simulationResult, setSimulationResult] = useState(null);
   ```

3. **Integrate into SimulationPanel:**
   - The `SimulationPanel` component already calls the API
   - It should pass results to `ArtifactPreviewPanel`:
     ```typescript
     <ArtifactPreviewPanel artifacts={simulationResult?.simulation?.artifacts} />
     ```

4. **Update layout:**
   - Current: Grid with 2 columns (rule visualization + simulation)
   - New: Grid with 3 columns or 2 rows (add artifact panel on right)
   - Use responsive design: panel collapses on small screens

5. **Test integration:**
   - Go to `/orchestrator` page
   - Create/select a rule
   - Click "Simulate"
   - Verify: Artifact preview panel appears on right with tabs
   - Check: Code, HTML, JSON, SQL tabs show content

**Files to modify:**
- `/app/orchestrator/page.tsx` (import components, add state, update layout)
- `/components/aei/simulation-panel.tsx` (ensure it passes artifacts to preview panel)

---

#### **Step 3: End-to-End Testing** (5 min)

**Goal:** Verify complete feature works from UI

### **Quick Test (5 min)**

1. **Start dev server:**
   ```bash
   npm run dev
   ```
   Wait for "ready - started server on 0.0.0.0:3000"

2. **Open browser:**
   Navigate to: `http://localhost:3000/orchestrator`

3. **Create a rule:**
   - Click **"New Rule"** button (left sidebar)
   - Fill in:
     - **Name:** `test-rule`
     - **Priority:** `5`
   - Click **Save**
   - ‚úÖ Rule appears in sidebar with P5 badge

4. **Select the rule:**
   - Click on the **`test-rule`** card in left sidebar (blue highlight shows it's selected)
   - Center area shows: "No artifacts yet. Run simulation to generate."
   - ‚úÖ Right panel shows **Simulation** section with controls

5. **Run simulation:**
   - In right panel, click **"Run Simulation"** button (blue button)
   - Wait 2-3 seconds for spinner to finish
   - Check **right sidebar** - artifact preview panel should appear with tabs:
     - ‚úÖ **Code** - Python function with syntax highlighting
     - ‚úÖ **HTML** - UI mockup rendered live
     - ‚úÖ **JSON** - Schema with tree view
     - ‚úÖ **SQL** - CREATE TABLE statement
     - ‚úÖ **Config** - Docker compose snippet

6. **Test artifact features:**
   - ‚úÖ Click **Code** tab ‚Üí see syntax highlighting
   - ‚úÖ Click **Copy** button ‚Üí paste into notepad (verify exact match)
   - ‚úÖ Click **Export** ‚Üí file downloads as `.py`
   - ‚úÖ Change **Max Agents** slider ‚Üí click "Run Simulation" again
   - ‚úÖ Artifacts regenerate with new content
   - ‚úÖ Each artifact has validation badge (green ‚úÖ)

### **If panel doesn't appear:**
1. Open **F12** (browser console)
2. Check for JavaScript errors
3. Look for debug logs: `[decompose]`, `[assign]`, `[simulate]`
4. Check Network tab - ensure `/api/orchestrator/simulate` returns `200`

---

### Testing Approach (Until UI is Integrated)

**Use API directly while UI integration is in progress:**

**Option 1: PowerShell (Windows/WSL)**
```powershell
# Start dev server first
npm run dev

# In another terminal, run test script:
pwsh -File test-artifact-workflow.ps1
```

**Option 2: curl (Windows PowerShell or WSL bash)**
```bash
# Create test file: test-orchestrator.json (see example below)

# Send request:
curl -s -X POST http://localhost:3000/api/orchestrator/simulate \
  -H "Content-Type: application/json" \
  -d @test-orchestrator.json | jq .
```

**Example test-orchestrator.json:**
```json
{
  "instruction_graph": [
    {
      "id": "task-1",
      "type": "code_gen",
      "description": "Generate API",
      "estimated_cost": 10,
      "estimated_duration": 30
    },
    {
      "id": "task-2",
      "type": "test_gen",
      "description": "Write tests",
      "estimated_cost": 5,
      "estimated_duration": 20,
      "dependencies": ["task-1"]
    }
  ],
  "rule_set_id": "default"
}
```

---

### Debugging Commands

**Check if dev server is running:**
```bash
curl http://localhost:3000 | head -20
```

**View API response (formatted):**
```bash
# Linux/WSL:
curl -s http://localhost:3000/api/orchestrator/simulate ... | jq .

# Windows PowerShell:
$resp = Invoke-WebRequest -Uri "http://localhost:3000/api/orchestrator/simulate" ...
$resp.Content | ConvertFrom-Json | ConvertTo-Json
```

**Enable Next.js debug logging:**
```bash
# Add to .env.local:
DEBUG=next:*
```

**Check TypeScript errors:**
```bash
pnpm tsc --noEmit
```

---

## Testing Workflow (15 min)

### Test 1: Basic Artifact Preview via API
**Goal:** Verify artifacts generate correctly

**Option A: Use the automated test script (RECOMMENDED)**

Run the full workflow test:
```bash
cd /path/to/repo/ari
bash test-artifact-workflow.sh
```

This script will:
1. Load the sample canvas from `/public/sample-canvas.json`
2. Build an instruction graph with 5 tasks
3. Call `/api/orchestrator/simulate`
4. Display artifact breakdown with validation status

**Option B: Manual API test (Postman / curl)**

**Step 1: Prepare test request**

Use curl or Postman to send this request to `/api/orchestrator/simulate`:

```json
{
  "instruction_graph": {
    "nodes": [
      {
        "id": "task-1",
        "type": "code_gen",
        "label": "Generate Python Function",
        "metadata": { "language": "python" }
      },
      {
        "id": "task-2",
        "type": "test_gen",
        "label": "Generate Tests",
        "metadata": { "language": "python" }
      },
      {
        "id": "task-3",
        "type": "sql_migration",
        "label": "Create Database",
        "metadata": {}
      },
      {
        "id": "task-4",
        "type": "html_gen",
        "label": "Generate HTML UI",
        "metadata": {}
      },
      {
        "id": "task-5",
        "type": "deploy_config",
        "label": "Docker Config",
        "metadata": {}
      }
    ],
    "edges": [
      { "source": "task-1", "target": "task-2" },
      { "source": "task-1", "target": "task-3" },
      { "source": "task-3", "target": "task-5" }
    ]
  },
  "rule_set": {
    "agent_selection": "auto",
    "task_grouping": true
  },
  "constraints": {
    "max_agents": 3,
    "timeout_ms": 5000
  }
}
```

**Step 2: Send request and inspect response**

Look for in the JSON response:
- [ ] `artifacts` array is populated
- [ ] Each artifact has: `type`, `language`, `content`, `metadata`, `validation`
- [ ] Validation shows: `status` (valid/warning/error), `errors[]`, `warnings[]`

**Step 3 (If UI works): View artifacts in UI**
- [ ] Artifact preview panel appears on right sidebar
- [ ] Tabs exist for: Code, HTML, JSON, SQL, Config
- [ ] Code tab shows Python function with proper syntax highlighting
- [ ] HTML tab shows rendered mockup (or source code)
- [ ] JSON tab shows schema with tree view
- [ ] SQL tab shows CREATE TABLE statement
- [ ] Config tab shows docker-compose snippet

### Test 2: Validation Badges
**Goal:** Verify validation badges appear correctly

1. Same setup as Test 1
2. Look at each artifact:
   - [ ] All artifacts have validation badge (top-right)
   - [ ] Valid artifacts show ‚úÖ badge
   - [ ] Any with syntax issues show ‚ö†Ô∏è or ‚ùå
3. Click validation badge to see details (if present)

### Test 3: Copy & Export
**Goal:** Verify copy/export functionality

1. Open Code tab
2. Click "Copy" button
3. Verify: [ ] Text copied to clipboard (paste into text editor to confirm)
4. Click "Export as .py"
5. Verify: [ ] File downloads as `artifact.py`

### Test 4: Re-simulation & Diff Viewer
**Goal:** Verify diffs when simulation parameters change

1. Run initial simulation (Test 1 setup)
2. Note the Code artifact content
3. Adjust a constraint: e.g., change "max_agents" slider to different value
4. Click "Simulate" again
5. Verify:
   - [ ] Artifacts regenerate with different content
   - [ ] "Diff" button appears (or auto-shows diff view)
   - [ ] Diff view shows side-by-side comparison
   - [ ] Added lines highlighted in green
   - [ ] Removed lines highlighted in red
   - [ ] Diff summary shows "X lines added, Y removed, Z% changed"

### Test 5: Search & Filter
**Goal:** Verify artifact search works

1. Open artifact preview panel
2. Look for Search box (usually top of panel)
3. Type keyword: `def ` (search for function definitions)
4. Verify:
   - [ ] Search highlights matching Code artifacts
   - [ ] Only matching artifact types shown (if filter enabled)
   - [ ] Click result ‚Üí scrolls to artifact and highlights match
5. Try filter buttons:
   - [ ] "Code only" button hides non-code artifacts
   - [ ] "Errors only" shows only invalid artifacts
   - [ ] "Clear filters" resets view

### Test 6: Metadata & Size Info
**Goal:** Verify artifact metadata is useful

1. Click on any artifact tab
2. Look for metadata badge (usually near export button):
   - [ ] Shows artifact type (Code, HTML, JSON, etc.)
   - [ ] Shows language (Python, JavaScript, SQL, etc.)
   - [ ] Shows size (e.g., "2.3 KB")
   - [ ] Shows version (e.g., "v1")

---

## Common Issues & Fixes

### Issue: Artifact preview panel doesn't appear
**Fix:**
- Ensure simulation completes (check status spinner)
- Refresh page: `Ctrl+R`
- Check browser console for errors: `F12` ‚Üí Console tab

### Issue: Syntax highlighting looks wrong
**Fix:**
- Language detection may be off. Check metadata for detected language.
- Try exporting artifact and opening in IDE for comparison

### Issue: HTML preview shows blank or broken
**Fix:**
- HTML artifact may have relative URLs (sandboxed preview can't access). This is expected.
- Check HTML source tab to see actual code

### Issue: Copy/Export buttons don't work
**Fix:**
- Check browser console for errors
- Ensure artifact content <100KB (MVP limit)
- Try manual copy: select text in code viewer + Ctrl+C

### Issue: Diff shows no changes after re-simulation
**Fix:**
- Parameter change may not affect artifact (e.g., changing a non-critical constraint)
- Try changing a major parameter (max_agents, task_count, etc.)

### Issue: Search is slow with many artifacts
**Fix:**
- Filtering may take <500ms. Wait a moment for results.
- Limit artifacts by using type filter first

---

## Advanced: Validation Details

### Validation Levels

Each artifact type has specific validation:

| Type | Validation | Pass Criteria |
|------|-----------|--------------|
| Code (Python) | AST parse | Valid Python syntax |
| Code (JavaScript) | Syntax check | Valid JS (basic) |
| Code (SQL) | Parse check | Valid SQL keywords |
| HTML | Tag balance | Properly closed tags |
| JSON | JSON.parse | Valid JSON structure |
| Config (YAML) | Basic parse | Valid YAML format |

### Understanding Validation Badges

- **‚úÖ Valid (green):** Artifact passes all checks
- **‚ö†Ô∏è Warning (yellow):** Minor issues (e.g., unclosed tag, missing field) but still usable
- **‚ùå Error (red):** Major syntax error, artifact may fail execution

**Note:** Validation is MVP-level (lint-style checks). Full type checking comes in Phase 2.

---

## Dogfooding Checklist (End-to-End Test)

Use this checklist to verify the complete feature:

- [ ] **Open Orchestrator Hub** and load instruction graph (5‚Äì10 tasks)
- [ ] **Click "Simulate"** ‚Üí see spinner ‚Üí artifacts generate in <3s
- [ ] **Artifact preview panel appears** on right (40% width, collapsible)
- [ ] **Switch between tabs**: Code ‚Üí see Python function, HTML ‚Üí see mockup, JSON ‚Üí see schema
- [ ] **Validation badges visible**: green ‚úÖ for all valid artifacts
- [ ] **Copy-to-clipboard works**: Code tab ‚Üí Copy button ‚Üí paste into editor ‚Üí exact match
- [ ] **Export artifact works**: Code tab ‚Üí "Export as .py" ‚Üí file downloads
- [ ] **Re-simulate with different constraints** (adjust max_agents slider)
- [ ] **Artifacts regenerate** with new content
- [ ] **Click "Diff" tab** ‚Üí see side-by-side comparison (old vs. new)
- [ ] **Diff highlights visible**: green for added, red for removed lines
- [ ] **Search artifacts**: type "def " ‚Üí Code tab highlighted, other tabs hidden
- [ ] **Filter by type**: "Code only" ‚Üí hide HTML/JSON/SQL tabs
- [ ] **Filter by validation**: "Errors only" ‚Üí show only ‚ùå artifacts
- [ ] **Metadata badges visible**: artifact type, language, size, version
- [ ] **Performance**: Full artifact generation <3s, re-generate <2s
- [ ] **Responsive layout**: Drag to resize preview panel, panel collapses on small screens

---

## Performance Baselines

When testing, confirm these performance targets:

| Operation | Target | Observed |
|-----------|--------|----------|
| Generate artifacts (10 tasks) | <3s | ‚Äî |
| Re-generate with new params | <2s | ‚Äî |
| Render 20+ artifacts | No lag | ‚Äî |
| Search in 50+ artifacts | <500ms | ‚Äî |
| Diff computation (1000+ lines) | <500ms | ‚Äî |

---

## FAQ

**Q: What is a "mock artifact"?**  
A: For MVP, artifacts are realistic stubs with TODO placeholders (e.g., `def generate_report(): # TODO`). Real execution will replace these with actual output from agents.

**Q: Can I edit artifacts in the preview?**  
A: No, preview is read-only for MVP. Editing comes in Phase 2.

**Q: Can I import artifacts back to the canvas?**  
A: Not in MVP. This is a Should-Have feature (F04-SH-01).

**Q: Are artifacts cached?**  
A: Yes, in memory per execution. Persistent caching is a Should-Have feature (F04-SH-04).

**Q: What artifact types are supported?**  
A: 10 types: code, html, json, sql, config, test, markdown, svg, dockerfile, yaml.

---

## Next Steps After Testing

If all tests pass:
1. Mark Feature-04 as **COMPLETE** in task file
2. Feature-05 (AI Trace Viewer) can begin
3. Proceed to Feature-06 (Analytics Pane) planning

If bugs found:
1. File bug report in `/docs/tasks/feature-04-output-simulator.md` progress section
2. Implementation Agent fixes
3. Re-test after fix

---

## Related Documentation

- **Task File:** `/docs/tasks/feature-04-output-simulator.md`
- **Architecture:** `/docs/architecture/feature-04-architecture.md`
- **Feature-03 (dependency):** `/docs/on-boarding/feature-03-onboarding.md`
- **API Reference:** `/docs/architecture/api-routes.md` (artifact endpoints)
